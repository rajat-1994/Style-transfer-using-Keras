{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.misc import imsave\n",
    "\n",
    "from keras import backend\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#defining the height and width of the all the three images\n",
    "image_height = 512\n",
    "image_width = 512\n",
    "#loading our content image\n",
    "content_image_path = 'hugo.jpg'\n",
    "content_image = Image.open(content_image_path)\n",
    "#resizeing our content image\n",
    "content_image = content_image.resize((image_height,image_width))\n",
    "#printing our content image\n",
    "content_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#loading and resize our two style images\n",
    "style_image_path = 'wave.jpg'\n",
    "style_image_path2 = 'picasso.jpg'\n",
    "\n",
    "style_image = Image.open(style_image_path)\n",
    "style_image2 = Image.open(style_image_path2)\n",
    "\n",
    "style_image = style_image.resize((image_height,image_width))\n",
    "style_image2 = style_image2.resize((image_height,image_width))\n",
    "\n",
    "style_image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "style_image2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Converting image pixels into numpy array so that computationally suitable\n",
    "content_array = np.asarray(content_image, dtype='float32')\n",
    "#here we are adding an extra dimention to the image so that we can concatinate all three images together\n",
    "content_array = np.expand_dims(content_array, axis=0)\n",
    "print(content_array.shape)\n",
    "\n",
    "#Converting image pixels into numpy array so that computationally suitable\n",
    "style_array = np.asarray(style_image, dtype='float32')\n",
    "#here we are adding an extra dimention to the image so that we can concatinate all three images together\n",
    "style_array = np.expand_dims(style_array, axis=0)\n",
    "print(style_array.shape)\n",
    "\n",
    "#Converting image pixels into numpy array so that computationally suitable\n",
    "style_array2 = np.asarray(style_image2, dtype='float32')\n",
    "#here we are adding an extra dimention to the image so that we can concatinate all three images together\n",
    "style_array2 = np.expand_dims(style_array2, axis=0)\n",
    "print(style_array2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p>Here we will perform two important steps</p>\n",
    "<ol>\n",
    "<li>substracting the mean RGB values from each pixel of all the three images</li>\n",
    "<li>Converting images from RGB to BGR as VGG16 model takes input images in BGR format</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#substracting mean RGB values from each pixel\n",
    "content_array[:, :, :, 0] -= 103.939\n",
    "content_array[:, :, :, 1] -= 116.779\n",
    "content_array[:, :, :, 2] -= 123.68\n",
    "#Converting RGB to BGR\n",
    "content_array = content_array[:, :, :, ::-1]\n",
    "\n",
    "style_array[:, :, :, 0] -= 103.939\n",
    "style_array[:, :, :, 1] -= 116.779\n",
    "style_array[:, :, :, 2] -= 123.68\n",
    "style_array = style_array[:, :, :, ::-1]\n",
    "\n",
    "style_array2[:, :, :, 0] -= 103.939\n",
    "style_array2[:, :, :, 1] -= 116.779\n",
    "style_array2[:, :, :, 2] -= 123.68\n",
    "style_array2 = style_array2[:, :, :, ::-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(content_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Here we are defining these arrays to the tensorflow variable using keras' backend\n",
    "content_image = backend.variable(content_array)\n",
    "style_image = backend.variable(style_array)\n",
    "style_image2 = backend.variable(style_array2)\n",
    "#we are declaring a placeholder to store our output later\n",
    "combination_image = backend.placeholder((1, image_height, image_width, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#here we concatinate our images into a single tensor thats is suitable for processing by keras' vgg16 model \n",
    "input_tensor = backend.concatenate([content_image,\n",
    "                                    style_image,\n",
    "                                    style_image2,\n",
    "                                    combination_image], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p>Now we put our tensor into the vvg16 model</p>\n",
    "<p>vgg16 model is a 16 layer cnn which was prepared by oxford's visual geometry group which won imagnet challenge in 2015</p>\n",
    "<p>vgg16 file is 500mb file</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#here we dont want to classsify our image \n",
    "#so we set include_top as false\n",
    "#which remove vgg16's pooling and fully connected layers\n",
    "model = VGG16(input_tensor=input_tensor, weights='imagenet',\n",
    "              include_top=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#lokking into different layers of our model\n",
    "layers = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "layers\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#here we will divide our total loss function into three different parts:\n",
    "# 1. content loss\n",
    "# 2. style loss\n",
    "# 3. total variation loss\n",
    "content_weight = 0.025\n",
    "style_weight = 5.0\n",
    "total_variation_weight = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#initializing total weight with zero adn then we add above weight loss to i\n",
    "loss=backend.variable(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Content Loss</h3>\n",
    "\n",
    "<p>Content loss is determined from the activation of a  trained cnn. Both the output image and base image are run thorough a cnn , which give us a set of features maps for both.the loss at a single layer is the eulidean(L2) distance between trhe activation of the content image and the activations of the output image</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Here we are defining our content  loss function to calculate content loss of our model using keras backend\n",
    "def content_loss(content, combination):\n",
    "    return backend.sum(backend.square(combination - content))\n",
    "\n",
    "#In this line we are choosing the layer from our model for which we want to calculate our content loss\n",
    "layer_features = layers['block2_conv2']\n",
    "#loading activations of the content image\n",
    "content_image_features = layer_features[0, :, :, :]\n",
    "#loading activation of the output image\n",
    "combination_features = layer_features[3, :, :, :]\n",
    "\n",
    "loss += content_weight * content_loss(content_image_features,\n",
    "                                      combination_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h3>Style Loss</h3>\n",
    "<p>Calculating Style loss is litle complex than calculating content loss. First we pass the style image nad output image through a cnn  and observe their activation. But instead of comparing the raw activations directly, we add another step. For both the images we take the Gram matrix of the activations at each layer in the network , the resulting matrix contains the correlation between every pair of feature maps at layer for that image</p>\n",
    "\n",
    "<p>After calculating Gram matrix ,we can define the single loss at a single layer as the euclidean distance between the Gram matrices of the style and output image</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#defining gram martix to calculate style loss\n",
    "def gram_matrix(x):\n",
    "    features = backend.batch_flatten(backend.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = backend.dot(features, backend.transpose(features))\n",
    "    return gram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Calculating the style loss for first style image\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = image_height * image_width\n",
    "    return backend.sum(backend.square(S - C)) / (4. * (channels ** 2) * (size ** 2))\n",
    "#calcuating style for the second style image\n",
    "def style_loss2(style1, style2, combination):\n",
    "    S1 = gram_matrix(style1)\n",
    "    S2 = gram_matrix(style2)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size =image_height * image_width\n",
    "    return backend.sum(backend.square(S1 - C) + backend.square(S2 - C) ) / (4. * (channels ** 2) * (size ** 2))\n",
    "\n",
    "#selecting layers from which we take acivations\n",
    "feature_layers = ['block1_conv2', 'block2_conv2',\n",
    "                  'block3_conv3', 'block4_conv3',\n",
    "                  'block5_conv3']\n",
    "for layer_name in feature_layers:\n",
    "    layer_features = layers[layer_name]\n",
    "    style_features = layer_features[1, :, :, :]\n",
    "    style_features2 = layer_features[2, :, :, :]\n",
    "    \n",
    "    combination_features = layer_features[3, :, :, :]\n",
    "    sl = style_loss2(style_features,style_features2, combination_features)\n",
    "    loss += (style_weight / len(feature_layers)) * sl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#to minimize noice in the output image\n",
    "def total_variation_loss(x):\n",
    "    a = backend.square(x[:, :image_height-1, :image_width-1, :] - x[:, 1:, :image_width-1, :])\n",
    "    b = backend.square(x[:, :image_height-1, :image_width-1, :] - x[:, :image_height-1, 1:, :])\n",
    "    return backend.sum(backend.pow(a + b, 1.25))\n",
    "\n",
    "loss += total_variation_weight * total_variation_loss(combination_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Defining gradient to optimaize our combination image\n",
    "\n",
    "grads = backend.gradients(loss, combination_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p>\n",
    "\n",
    "We then introduce an Evaluator class that computes loss and gradients in one pass while retrieving them via two separate functions, loss and grads. This is done because scipy.optimize requires separate functions for loss and gradients, but computing them separately would be inefficient.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "outputs = [loss]\n",
    "outputs += grads\n",
    "f_outputs = backend.function([combination_image], outputs)\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    x = x.reshape((1, image_height, image_width, 3))\n",
    "    outs = f_outputs([x])\n",
    "    loss_value = outs[0]\n",
    "    grad_values = outs[1].flatten().astype('float64')\n",
    "    return loss_value, grad_values\n",
    "\n",
    "class Evaluator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p>\n",
    "\n",
    "Now we're finally ready to solve our optimisation problem. This combination image begins its life as a random collection of (valid) pixels, and we use the L-BFGS algorithm (a quasi-Newton algorithm that's significantly quicker to converge than standard gradient descent) to iteratively improve upon it.\n",
    "\n",
    "We stop after 10 iterations because the output looks good to me and the loss stops reducing significantly.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = np.random.uniform(0, 255, (1, image_height, image_width, 3)) - 128.\n",
    "\n",
    "iterations = 10\n",
    "\n",
    "for i in range(iterations):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
    "                                     fprime=evaluator.grads, maxfun=20)\n",
    "    print('Current loss value:', min_val)\n",
    "    end_time = time.time()\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#printing output image\n",
    "#converting BGR into RGB\n",
    "x = x.reshape((image_height,image_width, 3))\n",
    "x = x[:, :, ::-1]\n",
    "x[:, :, 0] += 103.939\n",
    "x[:, :, 1] += 116.779\n",
    "x[:, :, 2] += 123.68\n",
    "x = np.clip(x, 0, 255).astype('uint8')\n",
    "\n",
    "Image.fromarray(x)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
